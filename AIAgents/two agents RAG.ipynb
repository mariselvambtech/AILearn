{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec1ee59-c925-459d-a853-f7cf496aabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import chromadb\n",
    "\n",
    "\n",
    "import autogen\n",
    "from autogen import AssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bd5544-d149-47b4-baa7-eb37481e4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-4.1-nano']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accepted file formats for that can be stored in\n",
    "# a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS\n",
    "\n",
    "config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST.json\")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230d2d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted file formats for `docs_path`:\n",
      "['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf', 'mdx']\n"
     ]
    }
   ],
   "source": [
    "print(\"Accepted file formats for `docs_path`:\")\n",
    "print(TEXT_FORMATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b5a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        # Let's choose the Meta's Llama 3.1 model (model names must match Ollama exactly)\n",
    "        \"model\": \"llama3.1:latest\",\n",
    "        # We specify the API Type as 'ollama' so it uses the Ollama client class\n",
    "        \"api_type\": \"ollama\",\n",
    "        \"stream\": False,\n",
    "        #\"client_host\": \"http://192.168.0.1:11434\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4094b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create an AssistantAgent instance named \"assistant\"\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"cache_seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24316fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "\n",
    "# Extract text from local PDF files\n",
    "pdf_files = [\"book.pdf\"]\n",
    "#docs_content = [extract_text_from_pdf(file) for file in pdf_files]\n",
    "\n",
    "from autogen.retrieve_utils import TEXT_FORMATS,extract_text_from_pdf\n",
    "docs_content= extract_text_from_pdf(\"book.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RetrieveUserProxyAgent instance\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\"book.pdf\"],\n",
    "        \"docs_content\": docs_content,  # Provide the extracted content directly\n",
    "        \"chunk_token_size\": 200,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"vector_db\": \"chroma\",\n",
    "        \"overwrite\": True,\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff479643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File autogen-docs/book.pdf does not exist. Skipping.\n",
      "2025-07-23 13:40:38,997 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 0 chunks.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n",
      "VectorDB returns doc_ids:  [[]]\n",
      "\u001b[32mNo more context, will terminate.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (541dfce8-0195-4f94-be10-d0f1ece31a55): Termination message condition on agent 'assistant' met\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"summarize the content and use python code to any calculation given in the book\"\n",
    "chat_result = ragproxyagent.initiate_chat(assistant, message=ragproxyagent.message_generator, problem=qa_problem)\n",
    "#agentops.end_session(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777cd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
