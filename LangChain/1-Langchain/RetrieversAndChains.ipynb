{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dec4ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # Load environment variables from a .env file\n",
    "load_dotenv()  # Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab58dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\",\"\")  # Retrieve the OpenAI API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\",\"\")  # Retrieve the LangChain API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Enable LangChain tracing for debugging and monitoring\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\",\"\")  # Set the LangChain project name for organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab12a0",
   "metadata": {},
   "source": [
    "### Open AI\n",
    "- Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb2ca6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI  # Import the OpenAI chat model for use in LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Initialize the OpenAI chat model with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48310c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F08F0CD840>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F08F0CF010>, root_client=<openai.OpenAI object at 0x000001F08F0CD240>, root_async_client=<openai.AsyncOpenAI object at 0x000001F08F0CFE80>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee8f5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Mari! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-C2oRR1vm7MkKVomI8sCqqKETdhRfx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e1b6c188-ca50-476a-923d-70d4563ccaf2-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"my name is mari\")  # Test the model with a simple query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30456415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but as an AI I do not have the ability to know your personal information unless you have shared it with me during our conversation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 13, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C2oQTiA1JszLyINJMjrcGXgfNdrK0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9059f8c9-44e4-434a-8e28-573d2f838481-0', usage_metadata={'input_tokens': 13, 'output_tokens': 30, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI()  # Initialize the OpenAI chat model\n",
    "llm.invoke(\"do you know my name?\")  # Test the model with a simple query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668ad97",
   "metadata": {},
   "source": [
    "#### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed4c0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate  # Import the ChatPromptTemplate for creating chat prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e98d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='my name is mar', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure, I can help you with that. my name is mar', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Translate this to Tamil: my name is mar', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Sure, I can translate that to Tamil.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"assistant\", \"Sure, I can help you with that. {input}\"),\n",
    "    (\"human\", \"Translate this to {language}: {input}\"),\n",
    "    (\"assistant\", \"Sure, I can translate that to {language}.\"),\n",
    "])\n",
    "  \n",
    "prompt.invoke({\"language\":\"Tamil\",\"input\":\"my name is mar\"}).to_messages()  # Invoke the prompt with an empty context to test it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70624b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt|llm  # Create a chain that combines the prompt and the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "926d2ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"My name is Maariselvam\" to Tamil is \"என் பெயர் மாரிசெல்வம்\".', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 70, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-C2paTrgOOgAgXesZaLgGHVxNAoqgq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--84436756-b8f6-4898-aa05-a37c2c43e85a-0', usage_metadata={'input_tokens': 70, 'output_tokens': 25, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"language\":\"Tamil\",\"input\":\"my name is maariselvam\"})  # Invoke the chain with specific inputs to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63121405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input', 'language'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Sure, I can help you with that. {input}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'language'], input_types={}, partial_variables={}, template='Translate this to {language}: {input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='Sure, I can translate that to {language}.'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F08F14AEF0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F08F14B8B0>, root_client=<openai.OpenAI object at 0x000001F08F14B250>, root_async_client=<openai.AsyncOpenAI object at 0x000001F08F14A560>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), StrOutputParser(), StrOutputParser(), StrOutputParser(), StrOutputParser(), StrOutputParser()] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "## String output parse\n",
    "from langchain_core.output_parsers import StrOutputParser  # Import the string output parser for processing model responses\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = chain | parser  # Combine the chain with the string output parser\n",
    "print(chain)\n",
    "response = chain.invoke({\"language\":\"Tamil\",\"input\":\"my name is maariselvam\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "171426a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "என் பெயர் மாரிசெல்வம்.\n"
     ]
    }
   ],
   "source": [
    "print(response)  # Print the response from the chain after parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30467d61",
   "metadata": {},
   "source": [
    "#### Load data from a website and pass it to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cb01ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConcepts | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\\n\\nPrompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}\\n\\nThe playground UI will pick up is_logged_in variable, but any nested variables you\\'ll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\\n\\n{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.\\nStructured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.\\n\\nTags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "## load the site and scrape the content\n",
    "from langchain_community.document_loaders import WebBaseLoader # Import the web loader for scraping content from websites\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/prompt_engineering/concepts\")\n",
    "docs=loader.load()  # Load the content from the specified URL\n",
    "print(docs)  # Print the loaded documents to verify the content\n",
    "from langchain_core.documents import Document  # Import the Document class for handling documents in LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "50f5fdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Concepts | 🦜️🛠️ LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='behavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='and often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"The playground UI will pick up is_logged_in variable, but any nested variables you'll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Read the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Structured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Model\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Commits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Tags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Change the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "### split the docs to chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Import the text splitter for dividing documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=70)  # Initialize the text splitter with specific parameters\n",
    "docs = text_splitter.split_documents(docs)  # Split the loaded documents into chunks for processing\n",
    "print(docs) # Print the split documents to verify the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "983a2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F0C49DE9E0>, search_kwargs={}) middle=[] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "## apply the openai embedding model to the chunks\n",
    "from langchain_openai import OpenAIEmbeddings  # Import the OpenAI embeddings model for generating vector representations of text\n",
    "embeddings = OpenAIEmbeddings()\n",
    "## import the FAISS vectorStore for storing and retrieving embeddings\n",
    "from langchain_community.vectorstores import FAISS  # Import the FAISS vector store for efficient similarity search\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)  # Create a vector store from the split documents and embeddings\n",
    "vector_store_db = vectorstore.as_retriever()\n",
    "vector_store_parser = vector_store_db | parser  # Combine the vector store with the string output parser for processing\n",
    "print(vector_store_parser)  # Print the vector store parser to verify its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a7a2ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b3b2041d-29ae-40be-84aa-b762bc87fda0', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'),\n",
       " Document(id='79d3d0b2-d683-4569-8b5e-283298f7444b', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'),\n",
       " Document(id='2d4aa2b3-a198-4886-89a9-d715a3e7b0ff', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'),\n",
       " Document(id='ae333be2-50c1-4eb0-8d44-1ece09bb2f07', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_db.invoke(\"To test over a dataset\")  # Invoke the vector store with a query to retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adc03d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b3b2041d-29ae-40be-84aa-b762bc87fda0', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'),\n",
       " Document(id='79d3d0b2-d683-4569-8b5e-283298f7444b', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'),\n",
       " Document(id='2d4aa2b3-a198-4886-89a9-d715a3e7b0ff', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'),\n",
       " Document(id='ae333be2-50c1-4eb0-8d44-1ece09bb2f07', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"To test over a dataset\")  # Perform a similarity search in the vector store with a specific query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd945e1",
   "metadata": {},
   "source": [
    "#### Load the docs and pass to LLM without converting to vectors and store in DB\n",
    "- not that efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3cd00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConcepts | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\\n\\nPrompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}\\n\\nThe playground UI will pick up is_logged_in variable, but any nested variables you\\'ll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\\n\\n{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.\\nStructured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.\\n\\nTags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load the site and scrape the content\n",
    "from langchain_community.document_loaders import WebBaseLoader # Import the web loader for scraping content from websites\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/prompt_engineering/concepts\")\n",
    "docs=loader.load()  # Load the content from the specified URL\n",
    "print(docs)  # Print the loaded documents to verify the content\n",
    "from langchain_core.documents import Document  # Import the Document class for handling documents in LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6eee89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Concepts | 🦜️🛠️ LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='behavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='and often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"The playground UI will pick up is_logged_in variable, but any nested variables you'll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Read the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Structured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Model\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Commits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Tags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Change the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "### split the docs to chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Import the text splitter for dividing documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=70)  # Initialize the text splitter with specific parameters\n",
    "docs = text_splitter.split_documents(docs)  # Split the loaded documents into chunks for processing\n",
    "print(docs) # Print the split documents to verify the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96cae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Import the create_stuff_documents_chain for processing documents in LangChain\n",
    "## import chatprompt template for creating chat prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Import the ChatPromptTemplate for creating chat prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "151c78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question {question} based on the context provided.\n",
    "Context: {context}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6db0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create LLM\n",
    "from langchain_openai import ChatOpenAI  # Import the OpenAI chat model for use in LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # Initialize the OpenAI chat model with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0eb3597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question {question} based on the context provided.\\nContext: {context}'), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F17E80D5D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F17E80C940>, root_client=<openai.OpenAI object at 0x000001F0C4E5EBC0>, root_async_client=<openai.AsyncOpenAI object at 0x000001F17E80CC70>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)  # Create a document chain using the language model and the prompt\n",
    "print(chain)  # Print the chain to verify its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc3dbe45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing a dataset, as outlined in the context provided, involves several steps within the LangSmith platform. Here\\'s a concise guide based on the context:\\n\\n1. **Select the Dataset:** Begin by selecting the dataset you wish to test. This is typically done in the platform\\'s interface, where you can choose the dataset from the top right corner.\\n\\n2. **Start the Test:** Once the dataset is selected, initiate the test by pressing the \"Start\" button. This action will run your prompt or model against the entire dataset.\\n\\n3. **Configure Test Settings:**\\n   - **Streaming Results:** You have the option to choose whether the results are streamed back in real-time.\\n   - **Repetitions:** Set the number of repetitions for the test to ensure robust results. This might help in understanding the variability or consistency of the model’s outputs.\\n\\n4. **View the Experiment Results:** After the test concludes, you can click on the \"View Experiment\" button. This will allow you to delve deeper into the results, analyze the model’s performance, and make data-driven decisions.\\n\\n5. **Adjust and Iterate:** Based on the results, you may want to adjust your prompts, template settings, or model configurations to optimize performance. You can re-run the tests with these adjustments to see improvements.\\n\\nBy following these steps, you can effectively test a dataset within the LangSmith platform to evaluate and iterate your AI models and prompt engineering strategies.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"context\":docs,\n",
    "\n",
    "    \"question\":\"how to test a dataset?\"})  # Invoke the chain with a specific context to get a response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d3eeb",
   "metadata": {},
   "source": [
    "####  Load the docs and pass to LLM after converting to vectors and store in DB\n",
    " - ret_chain.invoke({\n",
    "    \"input\":\"how to test a dataset?\",\n",
    "   \n",
    "    })\n",
    "\n",
    "- in retrieval chain - the key should be always \"input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d7e0cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConcepts | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\\n\\nPrompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}\\n\\nThe playground UI will pick up is_logged_in variable, but any nested variables you\\'ll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\\n\\n{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.\\nStructured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.\\n\\nTags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]\n",
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Concepts | 🦜️🛠️ LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='behavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='and often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nTo add a conditional mustache prompt:\\n{{#is_logged_in}}  Welcome back, {{name}}!{{else}}  Please log in.{{/is_logged_in}}'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content=\"The playground UI will pick up is_logged_in variable, but any nested variables you'll need to specify yourself.\\nPaste the following into inputs to ensure the above conditional prompt works:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='{  \"name\": \"Alice\"}\\nMustache formatThe LangSmith Playground uses f-string as the default template format,\\nbut you can switch to mustache format in the prompt settings/template format section.\\nmustache gives you more flexibility around conditional variables, loops, and nested keys.\\nFor conditional variables, you\\'ll need to manually add json variables in the \\'inputs\\' section.\\nRead the documentation\\nTools\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Read the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Structured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Model\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Commits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Tags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Change the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'), Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load the site and scrape the content\n",
    "from langchain_community.document_loaders import WebBaseLoader # Import the web loader for scraping content from websites\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/prompt_engineering/concepts\")\n",
    "docs=loader.load()  # Load the content from the specified URL\n",
    "print(docs)  # Print the loaded documents to verify the content\n",
    "from langchain_core.documents import Document  # Import the Document class for handling documents in LangChain\n",
    "### split the docs to chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Import the text splitter for dividing documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=70)  # Initialize the text splitter with specific parameters\n",
    "docs = text_splitter.split_documents(docs)  # Split the loaded documents into chunks for processing\n",
    "print(docs) # Print the split documents to verify the chunks\n",
    "## apply the openai embedding model to the chunks\n",
    "from langchain_openai import OpenAIEmbeddings  # Import the OpenAI embeddings model for generating vector representations of text\n",
    "embeddings = OpenAIEmbeddings()\n",
    "## import the FAISS vectorStore for storing and retrieving embeddings\n",
    "from langchain_community.vectorstores import FAISS  # Import the FAISS vector store for efficient similarity search\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)  # Create a vector store from the split documents and embeddings\n",
    "vector_store_db = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dfc793a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Import the create_stuff_documents_chain for processing documents in LangChain\n",
    "## import chatprompt template for creating chat prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Import the ChatPromptTemplate for creating chat prompts\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question {input} based on the context provided.\n",
    "Context: {context}\"\"\"\n",
    ")\n",
    "### create LLM\n",
    "from langchain_openai import ChatOpenAI  # Import the OpenAI chat model for use in LangChain\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # Initialize the OpenAI chat model with specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d092c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the question {input} based on the context provided.\\nContext: {context}'), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F08F5DE350>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F08F5DE410>, root_client=<openai.OpenAI object at 0x000001F08F5DFA00>, root_async_client=<openai.AsyncOpenAI object at 0x000001F08F5DD540>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docchain = create_stuff_documents_chain(llm, prompt)  # Create a document chain using the language model and the prompt\n",
    "print(docchain)  # Print the chain to verify its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3290fbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To test a dataset, especially in the context of the LangSmith platform, you would follow these steps:\\n\\n1. **Select the Dataset**: In the prompt playground, you can choose the dataset you want to test against. This option is available at the top-right corner of the interface.\\n\\n2. **Start the Testing Process**: Once you have selected the dataset, press the \"Start\" button to initiate the testing process. \\n\\n3. **Adjust Streaming and Repetitions**: You can modify whether the results are streamed back in real-time and set how many repetitions you want for the test.\\n\\n4. **Analyze the Results**: After running the test, you can click on the \"View Experiment\" button. This will allow you to dive deeper into the results, such as comparing outputs and determining which prompt versions performed better.\\n\\nThis process allows you to efficiently test multiple prompts across a dataset, facilitating the analysis and optimization of prompt engineering efforts. This integration with a playground environment makes it easier to iterate and refine the prompts to align with desired outcomes.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docchain.invoke({\n",
    "    \"context\":docs,\n",
    "    \"input\":\"how to test a dataset?\"})  # Invoke the document chain with a specific context to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "16ed79a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bbffb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F08F609990>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the question {input} based on the context provided.\\nContext: {context}'), additional_kwargs={})])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F08F5DE350>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F08F5DE410>, root_client=<openai.OpenAI object at 0x000001F08F5DFA00>, root_async_client=<openai.AsyncOpenAI object at 0x000001F08F5DD540>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
      "  }) kwargs={} config={'run_name': 'retrieval_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "ret_chain = create_retrieval_chain(vector_store_db,docchain)\n",
    "print(ret_chain)  # Print the retrieval chain to verify its configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "131f5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ret_chain.invoke({\n",
    "    \"input\":\"how to test a dataset?\",\n",
    "   \n",
    "    })  # Invoke the retrieval chain with  question to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9216b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'how to test a dataset?', 'context': [Document(id='fae47f35-bd8c-4b50-8dd3-310e7880129e', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.'), Document(id='e0645159-935e-46ff-99e7-917e74ff38ab', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts'), Document(id='99f16dc8-0dd0-4d2f-bef9-0c20c178c12c', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='You can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousHow to use multiple messages in the playgroundNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityLangChain'), Document(id='505969d0-c022-4312-983b-50799d030bb5', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | 🦜️🛠️ LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='multiple promptsTesting over a datasetCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')], 'answer': 'To test a dataset, follow these steps based on the provided context:\\n\\n1. **Select the Dataset**: Navigate to the top right section of your interface and choose the dataset you wish to test.\\n\\n2. **Start the Test**: Click the \"Start\" button to initiate the testing process for the selected dataset.\\n\\n3. **Configure Test Settings**:\\n   - Decide if you want the results to be streamed back during the test.\\n   - Set the number of repetitions or iterations for the test according to your testing requirements.\\n\\n4. **View Experiment Results**: After the test is complete, click the \"View Experiment\" button to explore the results in detail. This feature allows you to analyze the outcomes and gain insights from the data test.\\n\\nThese steps provide a structured way to test your dataset, allowing for configuration and detailed analysis of results.'}\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7c4934ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test a dataset, follow these steps based on the provided context:\n",
      "\n",
      "1. **Select the Dataset**: Navigate to the top right section of your interface and choose the dataset you wish to test.\n",
      "\n",
      "2. **Start the Test**: Click the \"Start\" button to initiate the testing process for the selected dataset.\n",
      "\n",
      "3. **Configure Test Settings**:\n",
      "   - Decide if you want the results to be streamed back during the test.\n",
      "   - Set the number of repetitions or iterations for the test according to your testing requirements.\n",
      "\n",
      "4. **View Experiment Results**: After the test is complete, click the \"View Experiment\" button to explore the results in detail. This feature allows you to analyze the outcomes and gain insights from the data test.\n",
      "\n",
      "These steps provide a structured way to test your dataset, allowing for configuration and detailed analysis of results.\n"
     ]
    }
   ],
   "source": [
    "print(out.get(\"answer\"))  # Print the answer from the retrieval chain response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bc1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
