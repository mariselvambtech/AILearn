{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b10ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # Load environment variables from a .env file\n",
    "load_dotenv()  # Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0013097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\",\"\")  # Retrieve the OpenAI API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\",\"\")  # Retrieve the LangChain API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Enable LangChain tracing for debugging and monitoring\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\",\"\")  # Set the LangChain project name for organization\n",
    "os.environ[\"GROQ_API_KEY\"]= os.getenv(\"GROQ_API_KEY\",\"\")  # Retrieve the GROQ API key from the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84253d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import groq\n",
    "from langchain_groq import ChatGroq  # Import the GroqChat model for use in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c431a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x00000150706D5F00> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000150706D5330> model_name='openai/gpt-oss-20b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")  # Initialize the GroqChat model with specific parameters\n",
    "print(llm)  # Print the model details to verify initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19608704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fbc67c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke([{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Explain the importance of fast language models\"\n",
    "}])  # Invoke the GroqChat model with a specific input to get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6810771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Why “Fast” Language Models Matter\\n\\nLanguage models (LMs) have become the backbone of many AI products—from chatbots and search engines to real‑time translation and autonomous systems.  Speed—both inference latency (how fast a single request is answered) and throughput (how many requests per second a system can handle)—directly impacts the feasibility, usability, and economics of these applications.  Below is a deep dive into why fast LMs are essential, the trade‑offs involved, and how speed can be engineered.\\n\\n---\\n\\n### 1. User Experience & Human‑Computer Interaction\\n\\n| Use‑Case | Typical Latency Requirement | Why Speed is Critical |\\n|---------|----------------------------|----------------------|\\n| Chatbots / Virtual Assistants | < 200\\u202fms (per turn) | Users expect conversational flow; a 1‑second pause feels “dead.” |\\n| Real‑time Translation | < 500\\u202fms per sentence | Lingual communication in meetings, travel, or customer support must feel seamless. |\\n| Autonomous Vehicles | < 10\\u202fms per sensor update | Decision‑making must occur in milliseconds to avoid accidents. |\\n| Voice‑to‑Text (dictation) | < 300\\u202fms per phrase | Users type speech in real‑time; delays break the rhythm. |\\n\\n**Key takeaway:** Even a 50\\u202fms latency increase can lead to noticeable “lag” and degrade perceived quality.\\n\\n---\\n\\n### 2. Economic & Energy Efficiency\\n\\n| Metric | Impact of Speed |\\n|-------|----------------|\\n| **Operational Cost** | Faster inference means fewer GPU/TPU hours per request → lower cloud spend. |\\n| **Energy Consumption** | High‑throughput models can use more power; optimized, faster models reduce joules per token. |\\n| **Hardware Utilization** | Faster models allow more requests per core, improving utilization and ROI. |\\n\\nFast LMs are therefore a lever for both **cost savings** and **environmental sustainability**.\\n\\n---\\n\\n### 3. Scalability & Multi‑Tenant Systems\\n\\n- **Throughput** determines how many concurrent users a service can support.  \\n- **Cold‑start** latency is critical in multi‑tenant SaaS; a slow model forces longer warm‑up times, hurting SLAs.  \\n\\nFast LMs enable **elastic scaling**: you can keep the same hardware footprint while serving thousands of concurrent users.\\n\\n---\\n\\n### 4. Edge & Low‑Latency Deployment\\n\\n- **Mobile / IoT**: Devices often have limited compute and battery.  \\n- **Offline**: Some applications (e.g., military, remote areas) cannot rely on constant cloud connectivity.  \\n\\nFast, lightweight models allow on‑device inference, preserving privacy and eliminating network delays.\\n\\n---\\n\\n### 5. Safety & Mitigation of “Hallucinations”\\n\\n- **Prompt‑response loops**: A slow LM may allow a user to repeatedly query, increasing the chance of hallucinated content.  \\n- **Real‑time monitoring**: Fast inference facilitates rapid detection of toxic or disallowed outputs, enabling immediate mitigation.\\n\\n---\\n\\n## How Speed is Achieved\\n\\n| Technique | What it Does | Typical Impact |\\n|----------|-------------|---------------|\\n| **Model Compression** (pruning, sparsity) | Removes redundant weights | 2–5× speed, <10\\u202f% accuracy loss |\\n| **Quantization** (int8, float16) | Reduces precision | 4–8× speed, <1\\u202f% accuracy loss |\\n| **Knowledge Distillation** | Trains smaller student from a large teacher | 10–20× speed, minimal loss |\\n| **Efficient Architectures** (e.g., Performer, BigBird, Reformer) | Reduce self‑attention complexity | 5–10× speed, maintain quality |\\n| **Hardware Acceleration** (TPU, GPU, ASIC, FPGA) | Specialized kernels | 2–10× speed |\\n| **Batching / Queueing** | Process multiple requests together | 3–6× throughput |\\n| **Dynamic Prompt Trimming** | Shorten prompts on the fly | 10–30\\u202f% speedup |\\n\\n**Trade‑offs**: Aggressive compression can degrade rare‑word handling or long‑context understanding. The right balance depends on the target application.\\n\\n---\\n\\n## Practical Examples\\n\\n| Product | Model | Speed Optimizations | Latency / Throughput |\\n|--------|------|--------------------|---------------------|\\n| **ChatGPT** (OpenAI) | GPT‑4 | Quantized inference + GPU batching | 300\\u202fms per 200‑token prompt |\\n| **Google Assistant** | T5‑Lite | Distilled + int8 | 150\\u202fms on edge |\\n| **DeepL Translator** | Transformer‑XL | Sparse attention + batching | < 400\\u202fms per sentence |\\n| **Tesla Autopilot** | Vision‑LM | Real‑time inference on NVIDIA Xavier | 10\\u202fms per frame |\\n\\n---\\n\\n## Bottom‑Line Checklist for Fast LMs\\n\\n1. **Define the latency budget** per use‑case.\\n2. **Profile** the baseline model on target hardware.\\n3. **Apply compression** (pruning, quantization) while monitoring accuracy.\\n4. **Leverage efficient attention** or alternative architectures.\\n5. **Optimize inference pipeline** (batching, async I/O).\\n6. **Validate** on edge devices if needed.\\n7. **Measure** both latency and throughput under realistic load.\\n8. **Iterate**: small accuracy drops can be acceptable for huge speed gains.\\n\\n---\\n\\n### Final Thought\\n\\nFast language models are not just a performance nicety—they are a **strategic necessity** for modern AI services. They unlock real‑time interaction, enable cost‑effective scaling, reduce energy footprints, and open doors to edge deployment. Investing early in speed‑aware design (compression, efficient architecture, hardware acceleration) pays dividends across the entire product lifecycle.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content  # Access the content of the response to see the model's output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddceae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage  # Import message types for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f066435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "பிரான்சின் தலைநகரம் எது?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant. Translate the following to Tamil.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")      \n",
    "]  # Define a list of messages for the model to process\n",
    "response = llm.invoke(messages)  # Invoke the model with the defined messages\n",
    "\n",
    "print(response.content)  # Print the model's response to the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f83be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**.', additional_kwargs={'reasoning_content': 'We need to answer: \"What is the capital of France?\" The answer: Paris. Probably straightforward.'}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 78, 'total_tokens': 118, 'completion_time': 0.033277059, 'prompt_time': 0.006315378, 'queue_time': 0.045877101, 'total_time': 0.039592437}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_fd68d84dd3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c2bc89ef-4182-4019-8fa0-bd82d0624e70-0', usage_metadata={'input_tokens': 78, 'output_tokens': 40, 'total_tokens': 118})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(input=\"What is the capital of France?\")  # Invoke the model with a simple input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437297d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Mariselvam! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"my name is mariselvam\". We need to respond. There\\'s no explicit instruction. The conversation: user says \"my name is mariselvam\". We should greet or respond acknowledging. Probably respond with something like \"Nice to meet you, Mariselvam.\" No special instructions. Use friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 77, 'total_tokens': 167, 'completion_time': 0.075513972, 'prompt_time': 0.006238151, 'queue_time': 0.089635608, 'total_time': 0.081752123}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_fc8da5621e', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c0a93c0c-9c4c-4c1b-a0fa-58c44083c4d5-0', usage_metadata={'input_tokens': 77, 'output_tokens': 90, 'total_tokens': 167})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([\"my name is mariselvam\"])  # Invoke the model with a list of strings as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6b839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Mariselvam! How can I help you today?', additional_kwargs={'reasoning_content': 'The user says \"my name is mariselvam\". We need to respond. There\\'s no explicit instruction. The conversation: user says \"my name is mariselvam\". We should greet or respond acknowledging. Probably respond with something like \"Nice to meet you, Mariselvam.\" No special instructions. Use friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 77, 'total_tokens': 167, 'completion_time': 0.074986393, 'prompt_time': 0.005616386, 'queue_time': 0.050129032, 'total_time': 0.080602779}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_fd68d84dd3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f2902a55-38ff-4161-8196-46af9ed232cf-0', usage_metadata={'input_tokens': 77, 'output_tokens': 90, 'total_tokens': 167})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"my name is mariselvam\")  # Invoke the model with a single string input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16af7fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "## String output parse\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()  # Initialize a string output parser\n",
    "chain = llm|parser\n",
    "response = chain.invoke(\"What is the capital of France?\")  # Invoke the chain with a question\n",
    "print(response)  # Print the parsed response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb74cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do with chatprompttemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Import the ChatPromptTemplate for structured prompts\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])  # Create a chat prompt template with system and human messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd0c776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|llm|parser\n",
    "chain.invoke({\"question\": \"What is the capital of France?\"})  # Invoke the chain with a question using the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b173006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'text'], input_types={}, partial_variables={}, template='Translate {text} to {language}.'), additional_kwargs={})]) middle=[ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000150706D5F00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000150706D5330>, model_name='openai/gpt-oss-20b', model_kwargs={}, groq_api_key=SecretStr('**********'))] last=StrOutputParser()\n",
      "வணக்கம், நீங்கள் எப்படி இருக்கிறீர்கள்?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Import the ChatPromptTemplate for structured prompts\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Translate {text} to {language}.\")  # Create a chat prompt template from a simple string format    \n",
    "chain = prompt_template | llm | parser  # Create a processing chain with the prompt, model, and parser\n",
    "print(chain)  # Print the chain details to verify its structure\n",
    "# Invoke the chain with specific parameters for translation\n",
    "result = chain.invoke({\"text\": \"Hello, how are you?\", \"language\": \"Tamil\"})  # Invoke the chain with specific parameters for translation\n",
    "print(result)  # Print the result of the chain invocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bd9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
