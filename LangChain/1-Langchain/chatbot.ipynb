{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e5b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  # Load environment variables from a .env file\n",
    "load_dotenv()  # Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62f4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\",\"\")  # Retrieve the OpenAI API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\",\"\")  # Retrieve the LangChain API key from the environment variables\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Enable LangChain tracing for debugging and monitoring\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\",\"\")  # Set the LangChain project name for organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef54890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x0000014F5287B3A0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000014F528B1810> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "## import groq\n",
    "from langchain_groq import ChatGroq  # Import the GroqChat model for use in LangChain\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")  # Initialize the GroqChat model with specific parameters\n",
    "print(llm)  # Print the model details to verify initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ebbd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**.', additional_kwargs={'reasoning_content': 'The user asks: \"What is the capital of France?\" This is a straightforward factual question. The answer is Paris. Provide answer.'}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 78, 'total_tokens': 124, 'completion_time': 0.083180669, 'prompt_time': 0.004455878, 'queue_time': 0.049660902, 'total_time': 0.087636547}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--73de0e6e-6630-4116-a2da-da95d84569c3-0', usage_metadata={'input_tokens': 78, 'output_tokens': 46, 'total_tokens': 124})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage  # Import HumanMessage for creating messages in LangChain\n",
    "\n",
    "llm.invoke([\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of France?\"\n",
    "    )]\n",
    ")  # Invoke the model with a human message asking for the capital of France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c716b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**.', additional_kwargs={'reasoning_content': 'The user asks: \"what is the capital of France?\" Straightforward answer: Paris. Provide answer.'}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 78, 'total_tokens': 118, 'completion_time': 0.073112541, 'prompt_time': 0.004566683, 'queue_time': 0.043770968, 'total_time': 0.077679224}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3d8a1e87-3177-4f14-8d6e-ad93e92afb66-0', usage_metadata={'input_tokens': 78, 'output_tokens': 40, 'total_tokens': 118})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is the capital of France?\")  # Invoke the model with a string input directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d2d4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes— you told me your name is **John**. How can I help you today?', additional_kwargs={'reasoning_content': 'The user asks \"hey do you know my name?\" The assistant already knows from previous message that the user said \"Hey my name is John\". So answer: yes, you know it\\'s John. Also ask how can help.'}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 102, 'total_tokens': 176, 'completion_time': 0.135052987, 'prompt_time': 0.005935127, 'queue_time': 0.192191353, 'total_time': 0.140988114}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--8d9b66b2-7b74-4ffb-9501-f3cc846439a0-0', usage_metadata={'input_tokens': 102, 'output_tokens': 74, 'total_tokens': 176})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check if the model is able to remember the previous question or messaes\n",
    "from langchain_core.messages import HumanMessage, AIMessage  # Import HumanMessage and AIMessage for creating messages in LangChain\n",
    "llm.invoke([\n",
    "    HumanMessage(content=\"Hey my name is John\"),\n",
    "    AIMessage(content = \"Hi John, how can I help you?\"),\n",
    "    HumanMessage(content=\"hey do you know my name?\")\n",
    "])  # Invoke the model with a sequence of   messages to check if it remembers the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16f0a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will create a chatbot with messae history\n",
    "from langchain_core.messages import HumanMessage, AIMessage  # Import HumanMessage and AIMessage for creating messages in LangChain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory  # Import InMemoryChatMessageHistory for storing chat messages in memory\n",
    "from langchain_core.chat_history import  BaseChatMessageHistory  # Import the base class for chat message history\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough  # Import RunnablePassthrough for passing through messages\n",
    "\n",
    "\n",
    "\n",
    "store={}\n",
    "\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if(session_id not in store):\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history= RunnableWithMessageHistory(llm,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "565d0b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is John.', additional_kwargs={'reasoning_content': 'The user asks \"what is my name?\" The conversation earlier: user said \"Hey my name is John\". So answer: John. Should be short.'}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 325, 'total_tokens': 371, 'completion_time': 0.083236358, 'prompt_time': 0.013803132, 'queue_time': 0.045547448, 'total_time': 0.09703949}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--89be80c9-f006-4187-be69-0fb1b98464e2-0', usage_metadata={'input_tokens': 325, 'output_tokens': 46, 'total_tokens': 371})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "                            [\n",
    "    HumanMessage(content=\"what is my name?\"),\n",
    "    \n",
    "    ],config={\n",
    "    \"configurable\":{\"session_id\": \"12345\"}  # Unique identifier for the session\n",
    "    \n",
    "}\n",
    "    )  # Invoke the model with a sequence of messages and session configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ecc2bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don’t have any information about your name. If you’d like me to address you by a particular name, just let me know!', additional_kwargs={'reasoning_content': 'The user asks \"what is my name?\" We have no prior context. According to policy, we cannot infer or guess. Should respond that I don\\'t know. Provide suggestion to tell us.'}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 119, 'total_tokens': 195, 'completion_time': 0.141600863, 'prompt_time': 0.005936807, 'queue_time': 0.044875554, 'total_time': 0.14753767}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--7bb866dc-c64d-4397-b20c-62552d5d8df0-0', usage_metadata={'input_tokens': 119, 'output_tokens': 76, 'total_tokens': 195})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with_message_history.invoke(\n",
    "                            [\n",
    "    HumanMessage(content=\"what is my name?\"),\n",
    "    \n",
    "    ],config={\n",
    "    \"configurable\":{\"session_id\": \"1245\"}  # Unique identifier for the session\n",
    "    \n",
    "}\n",
    "    )  # Invoke the model with a sequence of messages and session configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1a1916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Got it—your name is **Mari**. If you’d like me to use a different name or spelling, just let me know!', additional_kwargs={'reasoning_content': 'The user says \"my name is mari\". The user is now providing their name. The assistant should respond acknowledging. Should use the name \"Mari\" (capitalized). The user wants to know what is my name? The user says \"my name is mari\". The assistant should respond with \"Your name is Mari\". Should be polite.'}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 161, 'total_tokens': 267, 'completion_time': 0.193463214, 'prompt_time': 0.007595733, 'queue_time': 0.043193734, 'total_time': 0.201058947}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_4a19b1544c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--952e9beb-7b5a-4125-aa0c-b5ea500a0411-0', usage_metadata={'input_tokens': 161, 'output_tokens': 106, 'total_tokens': 267})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "                            [\n",
    "    HumanMessage(content=\"my name is mari\"),\n",
    "    \n",
    "    ],config={\n",
    "    \"configurable\":{\"session_id\": \"1245\"}  # Unique identifier for the session\n",
    "    \n",
    "}\n",
    "    ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1374d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You’ve told me your name is **Mari**. If you’d like me to use a different name or spelling, just let me know!', additional_kwargs={'reasoning_content': 'The user asks again \"what is my name?\" The user previously said \"my name is mari\". So we should answer \"Your name is Mari.\" Also maybe ask if they want anything else.'}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 205, 'total_tokens': 283, 'completion_time': 0.140705228, 'prompt_time': 0.008974085, 'queue_time': 0.045720492, 'total_time': 0.149679313}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--43ffc152-ecb5-4eef-b37b-74f2ca455a1c-0', usage_metadata={'input_tokens': 205, 'output_tokens': 78, 'total_tokens': 283})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "                            [\n",
    "    HumanMessage(content=\"what  is my name?\"),\n",
    "    \n",
    "    ],config={\n",
    "    \"configurable\":{\"session_id\": \"1245\"}  # Unique identifier for the session\n",
    "    \n",
    "}\n",
    "    ) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
